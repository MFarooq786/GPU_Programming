{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMEa2Bp6DGdauRUuUGNqxHu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Day 1: Introduction to GPU Programming with Numba\n","Goal: Learn the basics of Numba and how to run simple GPU code.\n","\n","Tasks:\n","Understand CUDA Basics:\n","\n","Learn about CUDA architecture (threads, blocks, grids, memory hierarchy).\n","Numba enables GPU programming using the CUDA model, but you'll use Python syntax.\n","\n","Install Required Libraries (on Colab):\n","\n","!pip install numba\n","\n","Write a Simple GPU Kernel: Numba makes it easy to write GPU kernels (functions executed on the GPU). Hereâ€™s a simple example of adding two arrays on the GPU."],"metadata":{"id":"4gG2I2B--BQA"}},{"cell_type":"code","source":["import numpy as np\n","from numba import cuda\n","\n","# Define the GPU Kernel\n","\n","@cuda.jit\n","def add_arrays_kernel(a, b, c):\n","\n","  # Get the thread index\n","  idx = cuda.grid(1)\n","  if idx < a.size: # Make sure thread index is within bounds\n","    c[idx] = a[idx] + b[idx]\n","\n","# Create the input arrays\n","\n","N = 100000\n","a = np.ones(N, dtype=np.float32)\n","b = np.ones(N, dtype=np.float32)\n","c = np.zeros(N, dtype=np.float32)\n","\n","# Transfer the data to the device (GPU memory)\n","d_a = cuda.to_device(a)\n","d_b = cuda.to_device(b)\n","d_c = cuda.to_device(c)\n","\n","# Define the block and grid size\n","threads_per_block = 256\n","blocks_per_grid = (a.size + (threads_per_block -1)) // threads_per_block\n","\n","#launch the kernel\n","add_arrays_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n","\n","# Copy the result back to the host (CPU memory)\n","d_c.copy_to_host(c)\n","# verify the result\n","print(\"Result of first 10 elements:\", c[:10])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AkXdEfr09-K3","executionInfo":{"status":"ok","timestamp":1735404398917,"user_tz":300,"elapsed":2462,"user":{"displayName":"Muhammad Farooq","userId":"04330857370022123958"}},"outputId":"c06f9ae3-831d-4474-c0dc-49612357df57"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Result of first 10 elements: [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"]}]},{"cell_type":"markdown","source":["@cuda.jit: This decorator is used to compile a Python function into a GPU kernel. It allows the function to be executed on the GPU rather than the CPU.\n","\n","cuda.grid(1): This function retrieves the unique thread index in a 1D grid of threads. It helps to ensure that each thread works on a specific element of the array.\n","\n","cuda.to_device: This function is used to transfer data from the CPU to the GPU memory. It prepares the data for computation on the GPU.\n","\n","add_arrays_kernel[blocks_per_grid, threads_per_block]: This line launches the GPU kernel. It specifies the number of blocks and threads per block in the 1D grid, which determines how the GPU will divide and distribute the work across its threads."],"metadata":{"id":"c_w3Qye8FScp"}},{"cell_type":"code","source":[],"metadata":{"id":"aZ5lb1rhFVxI"},"execution_count":null,"outputs":[]}]}
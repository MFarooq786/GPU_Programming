{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeYYEtQIo2WE2sTcYH7ZOx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Day 2: Understanding GPU Memory and Launch Configuration\n","Goal: Learn about different types of GPU memory (global, shared, and local) and configure the kernel execution parameters.\n","\n","Tasks:\n","Understand GPU Memory Types:\n","\n","**Global Memory**: This is the largest and slowest memory on the GPU. It’s used to store data that needs to be accessed by multiple threads.\n","Shared Memory: This is faster memory shared by threads within the same block. It's used to optimize performance for threads within a block.\n","Local Memory: This is memory local to each thread, but it is slower than shared memory and used when registers are insufficient.\n","Take note: For the task today, you’ll mainly work with global and shared memory.\n","\n","Write a Kernel to Use Shared Memory:\n","\n","We will create a kernel that uses shared memory to speed up computation (a simple summation of elements in an array).\n","The goal is to use shared memory to store intermediate results that can be accessed quickly by threads within the same block."],"metadata":{"id":"ZDx-r5WeLg15"}},{"cell_type":"code","source":["import numpy as np\n","from numba import cuda, float32\n","\n","@cuda.jit\n","def sum_kernel(arr, result):\n","    # Define shared memory within a block\n","    shared = cuda.shared.array(256, dtype=float32)\n","\n","    # Get the thread index\n","    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n","\n","    # Load data into shared memory\n","    if idx < arr.size:\n","        shared[cuda.threadIdx.x] = arr[idx]\n","    else:\n","        shared[cuda.threadIdx.x] = 0\n","    cuda.syncthreads()  # Synchronize threads before computation\n","\n","    # Perform a parallel reduction within the block\n","    stride = 1\n","    while stride < cuda.blockDim.x:\n","        if cuda.threadIdx.x % (2 * stride) == 0:\n","            shared[cuda.threadIdx.x] += shared[cuda.threadIdx.x + stride]\n","        stride *= 2\n","        cuda.syncthreads()\n","\n","    # The first thread in the block writes the result to global memory\n","    if cuda.threadIdx.x == 0:\n","        result[cuda.blockIdx.x] = shared[0]\n","\n","# Initialize data\n","N = 1000000\n","arr = np.ones(N, dtype=np.float32)\n","result = np.zeros((N // 256), dtype=np.float32)  # Result for each block\n","\n","# Transfer data to device\n","d_arr = cuda.to_device(arr)\n","d_result = cuda.to_device(result)\n","\n","# Define block and grid size\n","threads_per_block = 256\n","blocks_per_grid = (arr.size + threads_per_block - 1) // threads_per_block\n","\n","# Launch the kernel\n","sum_kernel[blocks_per_grid, threads_per_block](d_arr, d_result)\n","\n","# Copy the result back to host\n","d_result.copy_to_host(result)\n","\n","# Compute the final sum on the CPU by adding the results of each block\n","total_sum = np.sum(result)\n","print(\"Total sum:\", total_sum)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BFCxAw7eLpLR","executionInfo":{"status":"ok","timestamp":1735406602738,"user_tz":300,"elapsed":849,"user":{"displayName":"Muhammad Farooq","userId":"04330857370022123958"}},"outputId":"2dcf80ce-5ec4-4aa5-b960-b844a316ea14"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sum: 999936.0\n"]}]},{"cell_type":"markdown","source":["---\n","\n","\n","1. Import Libraries\n","*   numpy: A library for numerical computations, used here to create and manipulate arrays.\n","*   numba.cuda: Provides functionality for GPU programming using Numba's CUDA support.\n","*   float32: This is the data type we use for the shared memory array in the kernel.\n","\n","\n","---\n","\n","2. Define the GPU Kernel\n","*  @cuda.jit: This decorator compiles the sum_kernel function into a GPU kernel, making it ready for execution on the GPU.\n","*  cuda.shared.array(256, dtype=float32): Creates a shared memory array of size 256, where 256 is the number of threads per block. This memory is faster than global memory and is shared by all threads within a block.\n","\n","\n","---\n","\n","3. Thread Indexing\n","* cuda.threadIdx.x: Index of the current thread within its block (0 to blockDim.x - 1).\n","* cuda.blockIdx.x: Index of the current block in the grid (0 to gridDim.x - 1).\n","* cuda.blockDim.x: The number of threads in a block.\n","\n","The overall index idx is calculated as threadIdx + blockIdx * blockDim, giving each thread a unique index across all blocks.\n","\n","\n","---\n","\n","4. Load Data into Shared Memory\n","* Loading Data: Each thread loads an element from the global memory array arr into the shared memory shared.\n","* Index Bound Check: We check if the thread index idx is within the array size to avoid accessing out-of-bounds elements.\n","* cuda.syncthreads(): Synchronizes all threads in the block to ensure that all threads have finished loading data before any computation starts.\n","\n","---\n","5. Parallel Reduction\n","* Parallel Reduction: This technique is used to sum the elements in shared memory. We repeatedly sum pairs of elements (at a distance of stride), reducing the array size by half with each iteration. This continues until only one value remains.\n","  * cuda.threadIdx.x % (2 * stride) == 0: This condition ensures that only threads with indices that are divisible by 2 * stride participate in the reduction step.\n","  * shared[cuda.threadIdx.x] += shared[cuda.threadIdx.x + stride]: Threads sum pairs of elements in the shared memory.\n","  * stride *= 2: Double the stride after each iteration to sum larger gaps.\n","* Synchronization: cuda.syncthreads() ensures that all threads in the block finish their operations before moving to the next step.\n","\n","---\n","6. Write Final Result\n","*  After the parallel reduction, the first thread in each block (cuda.threadIdx.x == 0) writes the reduced sum from the shared memory (shared[0]) to the global memory result.\n","---\n","7. Initialize Data on Host (CPU)\n","* arr: An array of size N = 1,000,000 initialized with ones.\n","* result: An array that will store the partial sums from each block. The size of result is N // 256, as each block will compute one partial sum.\n","\n","---\n","8. Transfer Data to GPU\n","*  cuda.to_device(): Transfers the input data arr and result from the CPU (host) to the GPU (device) memory.\n","\n","---\n","\n","9. Launch the Kernel\n","* Launch Configuration:\n","    * threads_per_block = 256: Specifies the number of threads per block. Each block will have 256 threads.\n","    * blocks_per_grid: The number of blocks is calculated based on the size of arr. Since each block processes threads_per_block elements, we divide the total size of arr by threads_per_block and round up to ensure every element is processed.\n","* Kernel Launch: The kernel is launched with blocks_per_grid blocks and threads_per_block threads per block.\n","\n","---\n","10. Copy Result back to Host\n","*  This copies the computed partial results from the device back to the host (CPU) memory.\n","\n","---\n","11. Final Sum on CPU\n","* The CPU calculates the total sum by summing up the partial results from each block stored in result.\n","* np.sum(result): Sums all the partial results.\n","---\n","**Summary**\n","\n","* Shared Memory: Used for faster access within the block. We load data into shared memory and perform a parallel reduction to compute the sum more efficiently.\n","* Parallel Reduction: This technique allows threads in a block to collaborate and reduce the array size progressively, speeding up the summation process.\n","* Block and Grid Configuration: The kernel is launched with a grid of blocks, where each block processes a portion of the data using threads.\n","---"],"metadata":{"id":"uClk0TJXS1_l"}},{"cell_type":"code","source":[],"metadata":{"id":"cBr_LxBBT8wz"},"execution_count":null,"outputs":[]}]}